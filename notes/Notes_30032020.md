Register renaming -> Comment on peut casser les dependances en utilisant des registres suplémentaires dans la micro-architecture ?

hardware 101
1) Risk a du succès parce que instructions plus simple et standard, du coups plus facile d'écrire un compilateur. Cisc complet plus dur parce que beaucoup de dépendances.De plus
plus facile de faire du pipelining (fréquence d'horloge plus élévée)

2) c'est CISC, pas les deux, mais actuellement l'implémentation de l'isa se repose sur la décomposition des instrucions complexe en instructions plus simples (type RISC)

3) Intel est devenu successful car ils ont garder la comptabilité binaire, ainsi le jeu d'instruction evolue mais la retro comptabilité était OK, ce qui a permis un avantage pour Intel

4) Comme la deuxième question, décomposition des instructions complexes en instructions plus simple (type risc)



1) Cela permet d'utiliser par exemple 6 port (unité parallèle genre load, alu, etc...) en même temps afin d'occuper un max les coups d'horloges. Cela veut dire
qu'on a besoin d'un tampon conséquent permettant de récupérer les instructions dans le buffer pouvant être utilisées en parrallèle et out of order.

2) Même raison que ci dessus, on a besoin de toutes les données pour faire les traitements en parallèle, et donc on veut avoir les données en cache pour le principe de localité
afin de minimiser le temps de récupération des données pour executer les instructions en parallèle.

3) Car cela permet d'avoir deux coeur logiques sur un seul core physique, c'est a dire une seule partie traitement (ALU, LOAD , etc...) mais deux parties de traitement des instructions. Cela permet d'avoir un minimum d'overhead avec beaucoup d'instructions indépendantes. En effet , cela permet de traiter deux morceaux de codes indépendant en même temps sans trop souffrir des data dependencies.

Notes : Le fetch buffer permet d'avoir tout le temps suffisament de données afin de pouvoir décoder les instructions. En effet, la taille d'une instruction x86 est variable.

1) La latence devient critique lors de data dependency.

2) Le throughput devient plus important lorsque que on à pas de data dependency, et que l'algorithme est massivement parallèle. On veut dans ce cas la être capable d'effectuer un
maximum d'instructions à chaque coups d'horloge et en parallèle.

3) - Eviter les data dependences
   - Utilisation de la mémoire cache
   - Essayer d'avoir un minimum de branchements liés a des calculs dans notre système, et essayer de calculer les conditions le plus loin possible du branchement(ça aide le compilateur). Control de flux simple et prédictible.

4) Caches intégrée avec MMU, ou retrouve-t-on la MMU ? -> A partir de la cache L2, car on veut que la cache l1 soit la plus rapide possible, et on veut pas rajouter une traduction d'adresse avant d'aller chercher dans la cahe l1, ce qui introduirait de la latence. En parallèle de la recherche dans la cache l1, on peut regarder dans la TLB et enchainer sur la cache l2 si y'a pas eu dans la cache l1.

Question GAB chapitre 4 slide 102 : Speculation and exceptions -> C'est quoi ? -> Speculation = Capacité a effectuer du code sans savoir qu'il doit effectivement s'effectuer.
avec par exemple if(poit != NULL)faire quelque chose,  avec la speculation on attend pas le résultat et on effectue l'instruction après, et si cette instruction est la dereference du pointer, on veut pas avoir de segmentation fault. Du coups microarchitecture a un état interne compliqué et dans ce cas la , il faut masquer le contexte d'execution afin d'avoir la speculation et de fonctionner en parallèle. Ainsi si on a un code correcte , comme l'exemple plus haut, on peut avoir des exceptions, et cette slide explique comment et pourquoi.
